<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Evaluation metrics for generative models</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="main.tex" name="src"/>
<script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript"></script>
<link href="style.css" rel="stylesheet" type="text/css"/>
<link href="assets/icon.png" rel="icon" type="image/x-icon"/>
</head><body>
<nav class="TOC"><span class="mainToc"><a href="index.html"><img alt="Symmetries, QFT, &amp; The Standard Model" class="mainTocLogo" src="assets/logo.png" width="100%"/></a></span>
<span class="likepartToc"><a href="Frontmatter.html#front-matter">Front matter</a></span>
<span class="likepartToc"><a href="AbstractoftheDissertation.html#abstract-of-the-dissertation">Abstract of the Dissertation</a></span>
<span class="likepartToc"><a href="Introduction.html#introduction">Introduction</a></span>
<span class="partToc">I  <a href="TheoreticalBackground.html#theoretical-background">Theoretical Background</a></span>
<span class="partToc">II  <a href="ExperimentalBackground.html#experimental-background">Experimental Background</a></span>
<span class="partToc">III  <a href="AIMLandStatisticsBackground.html#aiml-and-statistics-background">AI/ML and Statistics Background</a></span>
<span class="partToc">IV  <a href="AcceleratingSimulationswithAI.html#accelerating-simulations-with-ai">Accelerating Simulations with AI</a></span>
<span class="chapterToc">9 <a href="IntroductionandtheJetNetDataset.html#introduction-and-the-jetnet-dataset">Introduction and the JetNet Dataset</a></span>
<span class="chapterToc">10 <a href="Generativemodelsforfastparticlecloudsimulations.html#generative-models-for-fast-particlecloud-simulations">Generative models for fast particle-cloud simulations</a></span>
<span class="chapterToc">11 <a href="Validatingandcomparingfastsimulations.html#validating-and-comparing-fast-simulations">Validating and comparing fast simulations</a></span>
<span class="sectionToc">11.1 <a href="#evaluation-metrics-for-generative-models">Evaluation metrics for generative models</a></span>
<span class="sectionToc">11.2 <a href="Experimentsongaussiandistributeddata.html#experiments-on-gaussiandistributed-data">Experiments on gaussian-distributed data</a></span>
<span class="sectionToc">11.3 <a href="Experimentsonjetdata.html#experiments-on-jet-data">Experiments on jet data</a></span>
<span class="sectionToc">11.4 <a href="DemonstrationonparticlecloudGANs.html#demonstration-on-particle-cloud-gans">Demonstration on particle cloud GANs</a></span>
<span class="sectionToc">11.5 <a href="Summary.html#summary4">Summary</a></span>
<span class="chapterToc">12 <a href="Conclusionandimpact.html#conclusion-and-impact">Conclusion and impact</a></span>
<span class="partToc">V  <a href="SearchesforHighEnergyHiggsBosonPairs.html#searches-for-high-energy-higgs-boson-pairs">Searches for High Energy Higgs Boson Pairs</a></span>
<span class="partToc">VI  <a href="AIforJets.html#ai-for-jets">AI for Jets</a></span>
<span class="partToc">VII  <a href="Appendix.html#appendix">Appendix</a></span>
<span class="likepartToc"><a href="Bibliography.html#bibliography">Bibliography</a></span>
</nav>
<main class="main-content"><a class="header-link" href="https://github.com/rkansal47/dissertation" rel="noopener noreferrer" style="top: 10px; right: 12px;" target="_blank"><img alt="GitHub Repository" class="header-icon" src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" style="width: 32px; height: 32px;"/></a><a class="header-link" href="https://github.com/rkansal47/dissertation/blob/gh-pages/dissertation.pdf?raw=true" rel="noopener noreferrer" style="top: 12px; right: 54px;" target="_blank"><img alt="Download PDF" class="header-icon" src="assets/download.png" style="width: 25px; height: 25px;"/></a>
<nav class="crosslinks-top"> <a href="Validatingandcomparingfastsimulations.html">⭠</a> <a href="Experimentsongaussiandistributeddata.html">⭢</a> </nav>
<h3 class="sectionHead" id="evaluation-metrics-for-generative-models"><span class="titlemark">11.1   </span> <a id="x54-23800011.1"></a>Evaluation metrics for generative models</h3>
<!--  l. 29  --><p class="noindent">In evaluating generative models, we aim to quantify the difference between the real and generated
data distributions <!--  l. 29  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
and <!--  l. 29  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> respectively,
where data samples <!--  l. 29  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle> <mo class="MathClass-rel" stretchy="false">∈</mo> <msup><mrow><mi>ℝ</mi></mrow><mrow><mi>d</mi></mrow></msup></mrow></math>
are typically high dimensional. Lacking tractable analytic distributions in general, this
can be viewed as a form of two-sample goodness-of-fit (GOF) testing of the hypothesis
<!--  l. 30  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> using real and
generated samples, <!--  l. 30  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo class="MathClass-open" stretchy="false">{</mo><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-close" stretchy="false">}</mo></mrow></math>
and <!--  l. 30  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo class="MathClass-open" stretchy="false">{</mo><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-close" stretchy="false">}</mo></mrow></math>,
drawn from their respective distributions. As illustrated in Ref. [<a href="Bibliography.html#Xcousins_gof">323</a>], in general,
there is no “best” GOF test with power against all alternative hypotheses.
Instead, we aim for a set of tests that collectively have power against the relevant
                                                                                

                                                                                
alternatives we expect, and are practically most appropriate. Below, we first outline
the criteria we require of our evaluation metrics, then review and discuss the
suitability of possible metrics, and end with a discussion on the features to use in
comparing such high-dimensional distributions, thereby motivating FPD and
KPD.
</p><!--  l. 35  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="criteria-for-evaluation-metrics-in-hep"><a id="x54-239000"></a>Criteria for evaluation metrics in HEP</h5>
<a id="x54-239000doc"></a>
<!--  l. 38  --><p class="noindent">Typical failure modes in ML generative models such as normalizing flows and
autoregressive models include a lack of sharpness and smearing of low-level features,
while generative adversarial networks (GANs) often suffer from “mode collapse”, where
they fail to capture the entire real distribution, only generating samples similar to a
particular subset. Therefore, with regard to the performance of generative
models, we require first and foremost that the tests be sensitive to both the
quality and the diversity of the generated samples. It is critical that these
tests are multivariate as well, particularly when measuring the performance of
conditional models, which learn conditional distributions given input features such as
those of the incoming particle into a calorimeter or originating parton of a
                                                                                

                                                                                
jet, and which will be necessary for applications to LHC simulations [<a href="Bibliography.html#XButter_2022rso">324</a>].
Multivariate tests are required in order to capture the correlations between
different features, including those on which such a model is conditioned. Finally,
it is desirable for the test’s results to be interpretable to ensure trust in the
simulations.
</p><!--  l. 45  --><p class="indent">       To facilitate a fair, objective comparison between generative models, we also
require the tests to be reproducible—i.e., repeating the test on a fixed set of
samples should produce the same result—and standardizable across different
datasets, such that the same test can be used for multiple classes and data
structures (e.g., both images and point clouds for calorimeter showers or jets). It
is also desirable for the test to be reasonably efficient in terms of speed and
computational resources, to minimize the burden on researchers evaluating their
models.
</p><!--  l. 48  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="evaluation-metrics"><a id="x54-240000"></a>Evaluation metrics</h5>
<a id="x54-240000doc"></a>
<!--  l. 51  --><p class="noindent">Having outlined criteria for our metrics, we now discuss possible metrics and their
merits and limitations. The traditional method for evaluating simulations in HEP is to
                                                                                

                                                                                
compare physical feature distributions using one-dimensional (1D) binned projections.
This allows valuable, interpretable insight into the physics performance of these
simulators. However, it is intractable to extend this binned approach to multiple
distributions simultaneously, as it falls victim to the curse of dimensionality—the
number of bins and samples required to retain a reasonable granularity in our
estimation of the multidimensional distribution grows exponentially with the
number of dimensions. Therefore, while valuable, this method is restricted to
evaluating single features, losing sensitivity to correlations and conditional
distributions.
</p>
<!--  l. 59  --><p class="noindent"><span class="paragraphHead" id="integral-probability-metrics-and-fdivergences"><a id="x54-241000"></a><span class="ec-lmbx-12">Integral  probability  metrics  and</span>
<!--  l. 59  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math><span class="ec-lmbx-12">-divergences</span></span>
<a id="x54-241000doc"></a>       
To extend to multivariate distributions, we first review measures of
differences between probability distributions. The two prevalent, almost mutually
exclusive,<span class="footnote-mark"><a href="#fn53x12" id="fn53x12-bk"><sup class="textsuperscript">1</sup></a></span><a id="x54-241001f1"></a>
classes of discrepancy measures are integral probability metrics (IPMs) [<a href="Bibliography.html#Xmuller_ipms">326</a>] and
<!--  l. 63  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences.
An IPM <!--  l. 64  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>D</mi></mrow><mrow><mstyle mathvariant="script"><mi>F</mi></mstyle></mrow></msub></math>,
defined as  </p><table class="equation"><tr><td>
<!--  l. 66  --><p class="indent">
</p><!--  l. 66  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x54-241003r1"></mstyle><!--  endlabel  -->
<msub><mrow><mi>D</mi></mrow><mrow><mstyle mathvariant="script"><mi>F</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-punc" stretchy="false">,</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo><munder class="msub"><mrow><mi class="qopname"> sup</mi><mo> ⁡<!--  FUNCTION APPLICATION  --> </mo> </mrow><mrow><mi>f</mi><mo class="MathClass-rel" stretchy="false">∈</mo><mstyle mathvariant="script"><mi>F</mi></mstyle></mrow></munder><mo class="MathClass-rel" stretchy="false">|</mo><msub><mrow><mi>𝔼</mi></mrow><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-rel" stretchy="false">∼</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub></mrow></msub><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-bin" stretchy="false">−</mo> <msub><mrow><mi>𝔼</mi></mrow><mrow><mstyle mathvariant="bold"><mi>y</mi></mstyle><mo class="MathClass-rel" stretchy="false">∼</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub></mrow></msub><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>y</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo><mo class="MathClass-rel" stretchy="false">|</mo><mo class="MathClass-punc" stretchy="false">,</mo>
</mrow></math></td><td class="eq-no">(11.1.1)</td></tr></table>
<!--  l. 68  --><p class="noindent">measures the difference in two distributions,
<!--  l. 69  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub></math> and
<!--  l. 69  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub></math> in Eq. (<a href="#x54-241003r1">11.1.1<!--  tex4ht:ref: eqn:ipm   --></a>), by using
a “witness” function <!--  l. 69  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>,
out of a class of measurable, real-valued functions
<!--  l. 69  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathvariant="script"><mi>F</mi></mstyle></math>, which
maximizes the absolute difference in its expected value over the two distributions. The
choice of <!--  l. 70  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathvariant="script"><mi>F</mi></mstyle></math>
defines different types of IPMs. The famous Wasserstein 1-distance
(<!--  l. 71  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub></math>) [<a href="Bibliography.html#Xwasserstein_original">327</a>, <a href="Bibliography.html#Xvillani_ot">328</a>], for example, is an
IPM for which <!--  l. 71  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathvariant="script"><mi>F</mi></mstyle></math> in Eq. (<a href="#x54-241003r1">11.1.1<!--  tex4ht:ref: eqn:ipm   --></a>)
is the set of all <!--  l. 71  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math>-Lipschitz
functions (where <!--  l. 71  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math>
is any positive constant). Maximum mean discrepancy (MMD) [<a href="Bibliography.html#Xgretton_mmd">329</a>] is another popular example,
                                                                                

                                                                                
where <!--  l. 72  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathvariant="script"><mi>F</mi></mstyle></math>
is the unit ball in a reproducing kernel Hilbert space (RKHS).
</p><!--  l. 74  --><p class="indent"> <!--  l. 74  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences,
on the other hand, are defined as </p><table class="equation"><tr><td>
<!--  l. 75  --><p class="indent">
</p><!--  l. 75  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x54-241004r2"></mstyle><!--  endlabel  -->
<msub><mrow><mi>D</mi></mrow><mrow><mi>f</mi></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-punc" stretchy="false">,</mo><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo><mo> ∫
 <!--  nolimits  --></mo><!--  nolimits  --><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi><mstyle mathsize="2.03em"><mrow><mo fence="true" form="prefix">(</mo><mrow></mrow><mo fence="true" form="postfix"></mo></mrow></mstyle><mfrac><mrow><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo></mrow>
<mrow><msub><mrow>
<mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-close" stretchy="false">)</mo></mrow></mfrac><mstyle mathsize="2.03em"><mrow><mo fence="true" form="prefix">)</mo><mrow></mrow><mo fence="true" form="postfix"></mo></mrow></mstyle><mi>d</mi><mstyle mathvariant="bold"><mi>x</mi></mstyle><mo class="MathClass-punc" stretchy="false">.</mo>
</mrow></math></td><td class="eq-no">(11.1.2)</td></tr></table>
<!--  l. 77  --><p class="noindent">They calculate the average of the pointwise differences between the two distributions,
<!--  l. 78  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub></math> and
<!--  l. 78  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub></math>
in Eq. (<a href="#x54-241003r1">11.1.1<!--  tex4ht:ref: eqn:ipm   --></a>), transformed by a “generating function”
<!--  l. 78  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>, weighted by
<!--  l. 78  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub></math>. Like IPMs,
different <!--  l. 79  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences
are defined by the choice of generating function. Famous examples include the
Kullback-Leibler (KL) [<a href="Bibliography.html#Xkl">330</a>] and Jenson-Shannon (JS) [<a href="Bibliography.html#Xjs_1">331</a>, <a href="Bibliography.html#Xjs_2">332</a>] divergences, which are
                                                                                

                                                                                
widely used in information theory to capture the expected information loss when modeling
<!--  l. 80  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>real</mi></mstyle></mrow></msub></math> by
<!--  l. 80  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>p</mi></mrow><mrow><mstyle mathvariant="normal"><mi>gen</mi></mstyle></mrow></msub></math> (or vice versa), as well
as the Pearson <!--  l. 80  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>χ</mi></mrow><mrow><mn>2</mn></mrow></msup></math> [<a href="Bibliography.html#Xpearson">333</a>]
divergence and related metrics [<a href="Bibliography.html#XBaker_1983tu">334</a>–<a href="Bibliography.html#Xparametric">336</a>], which are ubiquitous in HEP as GOF
tests.
</p><!--  l. 87  --><p class="indent">       Overall, <!--  l. 87  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences
can be powerful measures of discrepancies, with convenient information-theoretic
interpretations and the advantage of coordinate invariance. However, unlike IPMs, they
do not generally take into account the metric space of distributions, because of which we
argue that IPMs are more useful for evaluating generative models and their respective
learned distributions. An illustrative example of this is provided in Appendix <span class="ec-lmbx-12">??</span>. IPMs
can thereby be powerful metrics with which to compare different models, with measures
such as <!--  l. 90  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>W</mi></mrow><mrow><mn>1</mn></mrow></msub></math>
and MMD able to metrize the weak convergence of probability measures [<a href="Bibliography.html#Xvillani_ot">328</a>, <a href="Bibliography.html#Xsimongabriel_mmdmetrizing">337</a>].
</p><!--  l. 97  --><p class="indent">       Additionally, on the practical side, finite-sample estimation of
<!--  l. 97  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences such as
the KL and the Pearson <!--  l. 97  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>χ</mi></mrow><mrow><mn>2</mn></mrow></msup></math>
divergences is intractable in high dimensions, generally requiring partitioning
in feature space, which suffers from the curse of dimensionality as described
above. References [<a href="Bibliography.html#Xsriperumbudur_empirical">325</a>, <a href="Bibliography.html#Xsriperumbudur_ipms">338</a>] demonstrate more rigorously the efficacy of
finite-sample estimation of IPMs, in comparison to the difficulty of estimating
                                                                                

                                                                                
<!--  l. 98  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>-divergences.
</p>
<!--  l. 100  --><p class="noindent"><span class="paragraphHead" id="ipms-as-evaluation-metrics"><a id="x54-242000"></a><span class="ec-lmbx-12">IPMs as evaluation metrics</span></span>
<a id="x54-242000doc"></a>       
Having argued in their favor, we discuss specific IPMs and related measures,
and their viability as evaluation metrics. The most famous is the Wasserstein
distance [<a href="Bibliography.html#Xwasserstein_original">327</a>, <a href="Bibliography.html#Xvillani_ot">328</a>], as defined above. It is closely related to the problem of optimal
transport [<a href="Bibliography.html#Xvillani_ot">328</a>]: finding the minimum “cost” to transport the mass of one distribution to
another, when the cost associated with the transport between two points is the
Euclidean distance between them. This metric is sensitive to both the quality and
diversity of generated distributions; however, its finite-sample estimator is the
optimum of a linear program—an optimization problem with linear constraints and
objective [<a href="Bibliography.html#Xvanderbei2013linear">339</a>], which, while tractable in 1D, is biased with very poor convergence in
high dimensions [<a href="Bibliography.html#Xramdas_wasserstein">340</a>]. We demonstrate these characteristics empirically in
Sections <a href="Experimentsongaussiandistributeddata.html#experiments-on-gaussiandistributed-data">11.2<!--  tex4ht:ref: sec:04_evaluating_toydata   --></a> and <a href="Experimentsonjetdata.html#experiments-on-jet-data">11.3<!--  tex4ht:ref: sec:04_evaluating_jetdata   --></a>.
</p><!--  l. 109  --><p class="indent">       A related <i>pseudometric</i><span class="footnote-mark"><a href="#fn54x12" id="fn54x12-bk"><sup class="textsuperscript">2</sup></a></span><a id="x54-242001f2"></a>
is the Fréchet, or <!--  l. 109  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>W</mi></mrow><mrow><mn>2</mn></mrow></msub></math>,
distance between Gaussian distributions fitted to the features of interest, which we
generically call the Fréchet Gaussian distance (FGD). A form of this known as the
                                                                                

                                                                                
Fréchet <span class="small-caps">Inception</span> distance (FID) [<a href="Bibliography.html#XTTUR">311</a>], using the activations of the <span class="small-caps">Inception</span> v3
convolutional neural network model [<a href="Bibliography.html#Xinception_v3">341</a>] on samples of real and generated images as its
features, is currently the standard metric for evaluation in computer vision. The FID
has been shown to be sensitive to both quality and mode collapse in generative models
and is extremely efficient to compute; however, it has the drawback of assuming
Gaussian distributions for its features. While finite-sample estimates of the
FGD are biased [<a href="Bibliography.html#Xbinkowski_demystifying">342</a>], Ref. [<a href="Bibliography.html#Xchong_unbiasedfid">343</a>] introduces an effectively unbiased estimator
<!--  l. 112  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="normal"><mi>FGD</mi></mstyle></mrow><mrow><mi>∞</mi></mrow></msub></math>,
obtained by extrapolating from multiple finite-sample estimates to the infinite-sample
value.
</p><!--  l. 114  --><p class="indent">       The final IPM we discuss is the MMD [<a href="Bibliography.html#Xmmd">344</a>], for which
<!--  l. 114  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mstyle mathvariant="script"><mi>F</mi></mstyle></math> is the unit ball in an
RKHS for a chosen kernel <!--  l. 114  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>x</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>y</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>.
Intuitively, it is the distance between the mean embeddings of the two distributions in
the RKHS, and it has been demonstrated to be a powerful two-sample test [<a href="Bibliography.html#Xgretton_mmd">329</a>, <a href="Bibliography.html#Xliu_deepkernels">345</a>].
However, generally, high sensitivity requires tuning the kernel based on the two sets of
samples. For example, the traditional choice is a radial basis function kernel, where
kernel bandwidth is typically chosen based on the statistics of the two samples [<a href="Bibliography.html#Xgretton_mmd">329</a>].
While such a kernel has the advantage of being characteristic—i.e., it produces an
injective embedding [<a href="Bibliography.html#Xsriperumbudur_rkhs">346</a>]—to maintain a standard and reproducible metric, we
experiment instead with fixed polynomial kernels of different orders. These kernels allow
access to high order moments of the distributions and have been proposed in computer
vision as an alternative to FID, termed kernel <span class="small-caps">Inception</span> distance (KID) [<a href="Bibliography.html#Xbinkowski_demystifying">342</a>]. MMD
                                                                                

                                                                                
has unbiased estimators [<a href="Bibliography.html#Xgretton_mmd">329</a>], which have shown to converge quickly even in high
dimensions [<a href="Bibliography.html#Xbinkowski_demystifying">342</a>].
</p>
<!--  l. 123  --><p class="noindent"><span class="paragraphHead" id="manifold-estimation"><a id="x54-243000"></a><span class="ec-lmbx-12">Manifold estimation</span></span>
Another form of evaluation metrics recently popularized in computer vision
involves estimating the underlying manifold of the real and generated samples. While
computationally challenging, such metrics can be intuitive and allow us to disentangle
the aspects of quality and diversity of the generated samples, which can be valuable in
diagnosing individual failure modes of generative models. The most popular metrics are
“precision” and “recall” as defined in Ref. [<a href="Bibliography.html#Xkynkaanniemi_pr">347</a>]. For these, manifolds are first estimated
as the union of spheres centered on each sample with radii equal to the distance to the
<!--  l. 128  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math>th-nearest
neighbor. Precision is defined as the number of generated points which lie within the
real manifold, and recall as the number of real points within the generated manifold.
Alternatives, named diversity and coverage, are proposed in Ref. [<a href="Bibliography.html#Xnaeem_dc">348</a>] with a similar
approach, but which use only the real manifold, and take into account the density of the
spheres rather than just their union. We study the efficacy of both pairs of metrics for
our problem in Sections <a href="Experimentsongaussiandistributeddata.html#experiments-on-gaussiandistributed-data">11.2<!--  tex4ht:ref: sec:04_evaluating_toydata   --></a> and <a href="Experimentsonjetdata.html#experiments-on-jet-data">11.3<!--  tex4ht:ref: sec:04_evaluating_jetdata   --></a>.
                                                                                

                                                                                
</p>
<!--  l. 134  --><p class="noindent"><span class="paragraphHead" id="classifierbased-metrics"><a id="x54-244000"></a><span class="ec-lmbx-12">Classifier-based metrics</span></span>
Finally, an alternative class of GOF tests proposed in Refs. [<a href="Bibliography.html#Xliu_deepkernels">345</a>, <a href="Bibliography.html#Xfriedman_gof">349</a>, <a href="Bibliography.html#Xlopez_paz_c2st">350</a>], and
most relevantly in Ref. [<a href="Bibliography.html#Xkrause_caloflow">351</a>] and the fast calorimeter simulation challenge [<a href="Bibliography.html#Xcalochallenge">352</a>] to
evaluate simulated calorimeter showers, are based on binary classifiers trained between
real and generated data. These tests have been posited to have sensitivity
to both quality and diversity; however, they have significant practical and
conceptual drawbacks in terms of understanding and comparing generative
models.
</p><!--  l. 139  --><p class="indent">       First, deep neural networks (DNNs) are widely considered uninterpretable black
boxes [<a href="Bibliography.html#Xblack_box">353</a>], hence it is difficult to discern which features of the generated data the
network is identifying as discrepant or compatible. Second, the performance of DNNs is
highly dependent on both the architecture and dataset, and it is unclear how to specify
a standard architecture sensitive to all possible discrepancies for all datasets.
Furthermore, training of DNNs is typically stochastic, minimizing a complex loss
function with several potential local minima, and slow; hence it is sensitive to initial
states and hyperparameters irrelevant to the problem, difficult to reproduce, and not
efficient.
</p><!--  l. 143  --><p class="indent">       In terms of GOF testing, evaluating the performance of an individual
generative model requires a more careful understanding of the null distribution
of the test statistic than is proposed in Refs. [<a href="Bibliography.html#Xkrause_caloflow">351</a>, <a href="Bibliography.html#Xcalochallenge">352</a>], such as by using a
                                                                                

                                                                                
permutation test as suggested in Refs. [<a href="Bibliography.html#Xliu_deepkernels">345</a>, <a href="Bibliography.html#Xfriedman_gof">349</a>] or retraining the model numerous
times between samples from the true distribution as proposed recently in
Refs. [<a href="Bibliography.html#Xdagnolo_nplm">354</a>, <a href="Bibliography.html#Xdagnolo_lmnp">355</a>] with applications to HEP searches. However, even if such a test was
performed for each model, which would itself be practically burdensome, it
would remain difficult to fairly compare models, as, since different classifiers are
trained for each model, this means comparing values of entirely different test
statistics.<span class="footnote-mark"><a href="#fn55x12" id="fn55x12-bk"><sup class="textsuperscript">3</sup></a></span><a id="x54-244001f3"></a>
Despite these drawbacks, we perform the classifier-based test from Refs. [<a href="Bibliography.html#Xkrause_caloflow">351</a>, <a href="Bibliography.html#Xcalochallenge">352</a>] in
Section <a href="Experimentsonjetdata.html#experiments-on-jet-data">11.3<!--  tex4ht:ref: sec:04_evaluating_jetdata   --></a> and find that, perhaps surprisingly, it is insensitive to a large class of
failures typical of ML generative models.
</p><!--  l. 150  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="feature-selection"><a id="x54-245000"></a>Feature selection</h5>
<a id="x54-245000doc"></a>
<!--  l. 153  --><p class="noindent">We end by discussing which features to select for evaluation. Generally,
for data such as calorimeter showers and jets, individual samples
<!--  l. 154  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle> <mo class="MathClass-rel" stretchy="false">∈</mo> <msup><mrow><mi>ℝ</mi></mrow><mrow><mi>d</mi></mrow></msup></mrow></math>
are extremely high dimensional, with showers and jets containing up to
<!--  l. 154  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="script"><mi>O</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>1000</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>s
                                                                                

                                                                                
of hits and particles respectively, each with its own set of features.
Apart from the practical challenges of comparing distributions in this
<!--  l. 155  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math>-dimensional
case, often this full set of low-level features is not the most relevant for our downstream
use case.
</p><!--  l. 158  --><p class="indent">       This is an issue in computer vision as well, where images are similarly high
dimensional, and comparing directly the low-level, high-dimensional feature space of
pixels is not practical or meaningful. Instead, the current solution is to derive
salient, high-level features from the penultimate layer of a pretrained SOTA
classifier.
</p><!--  l. 161  --><p class="indent">       This approach is necessary for images, for which it is difficult to define such
meaningful numerical features by hand. We also tried a similar approach in Section <a href="MessagepassingGANs.html#message-passing-gans">10.1<!--  tex4ht:ref: sec:04_mpgan   --></a>
using the Fréchet ParticleNet distance (FPND), using the ParticleNet jet classifier to
derive its features. However, one key insight and study of this work is that this may be
unnecessary for HEP applications, as we have already developed a variety of meaningful,
hand-engineered features such as jet observables [<a href="Bibliography.html#XKomiske_2017aww">309</a>, <a href="Bibliography.html#Xmarzani_jets">356</a><span class="ec-lmbx-12">? </span>] and shower-shape
variables [<a href="Bibliography.html#Xbaffioni_electronrecocms">357</a>, <a href="Bibliography.html#Xatlas_photonrecoatlas">358</a>]. Such variables may lead to a more efficient, more easily
standardized, and interpretable test. We experiment with both types of features in
Section <a href="Experimentsonjetdata.html#experiments-on-jet-data">11.3<!--  tex4ht:ref: sec:04_evaluating_jetdata   --></a>.
                                                                                

                                                                                
</p>
<nav class="crosslinks-bottom"> <a href="Validatingandcomparingfastsimulations.html">⭠</a> <a href="Experimentsongaussiandistributeddata.html">⭢</a> </nav> <div class="footnotes"><a id="x54-241002x"></a>
<!--  l. 63  --><p class="indent"> <span class="footnote-mark"><a href="#fn53x12-bk" id="fn53x12"><sup class="textsuperscript">1</sup></a></span><span class="ec-lmr-10">The total variation distance is the only nontrivial discrepancy measure that is both an IPM and an</span>
<!--  l. 63  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math><span class="ec-lmr-10">-divergence [</span><a href="Bibliography.html#Xsriperumbudur_empirical"><span class="ec-lmr-10">325</span></a><span class="ec-lmr-10">,
Appendix A]; however, to our knowledge, a consistent finite-sample estimator for it does not exist (see,
for example, Ref. [</span><a href="Bibliography.html#Xsriperumbudur_empirical"><span class="ec-lmr-10">325</span></a><span class="ec-lmr-10">, Section 5]).</span></p><a id="x54-242002x"></a>
<!--  l. 109  --><p class="indent"> <span class="footnote-mark"><a href="#fn54x12-bk" id="fn54x12"><sup class="textsuperscript">2</sup></a></span><span class="ec-lmr-10">This is a pseudometric because distinct distributions can have a distance of 0 if they have
the same means and covariances.</span></p><a id="x54-244002x"></a>
<!--  l. 144  --><p class="indent"> <span class="footnote-mark"><a href="#fn55x12-bk" id="fn55x12"><sup class="textsuperscript">3</sup></a></span><span class="ec-lmr-10">In the case of Refs. [</span><a href="Bibliography.html#Xdagnolo_nplm"><span class="ec-lmr-10">354</span></a><span class="ec-lmr-10">, </span><a href="Bibliography.html#Xdagnolo_lmnp"><span class="ec-lmr-10">355</span></a><span class="ec-lmr-10">] the test statistic remains the same, but estimating the null
distribution is even more practically challenging, as it involves multiple trainings of the
classifier.</span></p> </div><div class="footer"><p>Copyright © 2024 Raghav Kansal. All rights reserved.</p></div></main>
</body>
</html>