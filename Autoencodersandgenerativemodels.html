<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Autoencoders and generative models</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="main.tex" name="src"/>
<script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript"></script>
<link href="style.css" rel="stylesheet" type="text/css"/>
<link href="assets/icon.png" rel="icon" type="image/x-icon"/>
</head><body>
<nav class="TOC"><span class="mainToc"><a href="index.html"><img alt="Symmetries, QFT, &amp; The Standard Model" class="mainTocLogo" src="assets/logo.png" width="100%"/></a></span>
<span class="likepartToc"><a href="Frontmatter.html#front-matter">Front matter</a></span>
<span class="likepartToc"><a href="AbstractoftheDissertation.html#abstract-of-the-dissertation">Abstract of the Dissertation</a></span>
<span class="likepartToc"><a href="Introduction.html#introduction">Introduction</a></span>
<span class="partToc">I  <a href="TheoreticalBackground.html#theoretical-background">Theoretical Background</a></span>
<span class="partToc">II  <a href="ExperimentalBackground.html#experimental-background">Experimental Background</a></span>
<span class="partToc">III  <a href="AIMLandStatisticsBackground.html#aiml-and-statistics-background">AI/ML and Statistics Background</a></span>
<span class="chapterToc">7 <a href="MachineLearningforHEP.html#machine-learning-for-hep">Machine Learning for HEP</a></span>
<span class="sectionToc">7.1 <a href="Introduction1.html#introduction1">Introduction</a></span>
<span class="sectionToc">7.2 <a href="Equivariantneuralnetworks.html#equivariant-neural-networks">Equivariant neural networks</a></span>
<span class="sectionToc">7.3 <a href="#autoencoders-and-generative-models">Autoencoders and generative models</a></span>
<span class="subsectionToc">7.3.1 <a href="#autoencoders-and-anomaly-detection">Autoencoders and anomaly detection</a></span>
<span class="subsectionToc">7.3.2 <a href="#generative-models">Generative models</a></span>
<span class="subsectionToc">7.3.3 <a href="#previous-work">Previous work</a></span>
<span class="chapterToc">8 <a href="DataAnalysisandStatisticalInterpretation.html#data-analysis-and-statistical-interpretation">Data Analysis and Statistical Interpretation</a></span>
<span class="partToc">IV  <a href="AcceleratingSimulationswithAI.html#accelerating-simulations-with-ai">Accelerating Simulations with AI</a></span>
<span class="partToc">V  <a href="SearchesforHighEnergyHiggsBosonPairs.html#searches-for-high-energy-higgs-boson-pairs">Searches for High Energy Higgs Boson Pairs</a></span>
<span class="partToc">VI  <a href="AIforJets.html#ai-for-jets">AI for Jets</a></span>
<span class="partToc">VII  <a href="Appendix.html#appendix">Appendix</a></span>
<span class="likepartToc"><a href="Bibliography.html#bibliography">Bibliography</a></span>
</nav>
<main class="main-content"><a class="header-link" href="https://github.com/rkansal47/dissertation" rel="noopener noreferrer" style="top: 10px; right: 12px;" target="_blank"><img alt="GitHub Repository" class="header-icon" src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" style="width: 32px; height: 32px;"/></a><a class="header-link" href="https://github.com/rkansal47/dissertation/blob/gh-pages/dissertation.pdf?raw=true" rel="noopener noreferrer" style="top: 12px; right: 54px;" target="_blank"><img alt="Download PDF" class="header-icon" src="assets/download.png" style="width: 25px; height: 25px;"/></a>
<nav class="crosslinks-top"> <a href="Equivariantneuralnetworks.html">⭠</a> <a href="DataAnalysisandStatisticalInterpretation.html">⭢</a> </nav>
<h3 class="sectionHead" id="autoencoders-and-generative-models"><span class="titlemark">7.3   </span> <a id="x41-1560007.3"></a>Autoencoders and generative models</h3>
<!--  l. 489  --><p class="noindent">
</p>
<h4 class="subsectionHead" id="autoencoders-and-anomaly-detection"><span class="titlemark">7.3.1   </span> <a id="x41-1570007.3.1"></a>Autoencoders and anomaly detection</h4>
<figure class="figure">
<!--  l. 495  --><p class="noindent" id="diagram-of-an-image-autoencoder-reproduced-from-ref-wengvae"><img alt="PIC" src="figures/03-ML/autoencoder-architecture.png" style="max-width:100%"/> <a id="x41-157001r11"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.11. </span></span><span class="content">Diagram of an image autoencoder, reproduced from Ref. [<a href="Bibliography.html#Xweng2018VAE">54</a>].    </span></figcaption><!--  tex4ht:label?: x41-157001r11   -->
</figure>
<!--  l. 500  --><p class="indent">       In this final section, we discuss two paradigms of unsupervised learning relevant
to this dissertation: autoencoders (AEs) and generative models. AEs are NN
architectures composed of an <i>encoder</i> network, which maps the input into
a typically lower dimensional latent space — called a “bottleneck” — and
a <i>decoder</i>, which attempts to reconstruct the original input from the latent
features (Figure <a href="#diagram-of-an-image-autoencoder-reproduced-from-ref-wengvae">7.11<!--  tex4ht:ref: fig:03_ml_autoencoder   --></a>). The bottleneck encourages AEs to learn a compressed
representation of data that captures salient properties [<a href="Bibliography.html#Xautoencoders">238</a>], which can be
valuable in HEP for compressing the significant volumes of data collected at the
LHC [<a href="Bibliography.html#XGuglielmo">239</a>].
</p><!--  l. 504  --><p class="indent">       The learned representation can also be exploited for later downstream tasks, such
as anomaly detection, where an autoencoder is trained to reconstruct data considered
“background” to our signal, with the expectation that it will reconstruct the signal
worse than the background. Thus, examining the reconstruction loss of a trained
autoencoder may allow the identification of anomalous data. This can be an advantage
in searches for new physics, since instead of having to specify a particular signal
hypothesis, a broader search can be performed for data incompatible with the
background. This approach has been successfully demonstrated in Refs. [<a href="Bibliography.html#XHeimel_2018mkt">240</a>–<a href="Bibliography.html#XDillon-lower-dimension">248</a>]. Two
recent exciting examples from CMS include a model-agnostic search for di-jet
resonances with Run 2 data [<a href="Bibliography.html#XCMS-PAS-EXO-22-026">249</a>], which prominently uses AEs for multiple search
strategies, and a new AE-based online Level-1 trigger paths implemented in Run
3 [<a href="Bibliography.html#XCMS-DP-2023-079">250</a>, <a href="Bibliography.html#XCMS-DP-2024-059">251</a>].
                                                                                

                                                                                
</p><!--  l. 510  --><p class="indent">       Furthermore, there are many possible variations to the general autoencoder
framework for alternative tasks [<a href="Bibliography.html#XAE-review-1">252</a>, <a href="Bibliography.html#XAE-review-2">253</a>], such as variational autoencoders
(VAEs) [<a href="Bibliography.html#XVAE">254</a>], which we discuss in the next section. While there have been some recent
efforts at GNN-based autoencoder models [<a href="Bibliography.html#XTsan_2021brw">62</a>, <a href="Bibliography.html#XAtkinson_2021nlt">255</a>], in this dissertation, we present the
first Lorentz-equivariant autoencoder for jets in Chapter <span class="ec-lmbx-12">??</span>. We focus on data
compression and anomaly detection but note that our model can be extended to further
applications, such as fast simulations in HEP.
</p>
<h4 class="subsectionHead" id="generative-models"><span class="titlemark">7.3.2   </span> <a id="x41-1580007.3.2"></a>Generative models</h4>
<figure class="figure">
<!--  l. 519  --><p class="noindent" id="summary-of-popular-generative-models-reproduced-from-ref-wengdiffusion"><img alt="PIC" src="figures/03-ML/generative-overview.png" style="max-width:100%"/> <a id="x41-158001r12"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.12. </span></span><span class="content">Summary of popular generative models, reproduced from Ref. [<a href="Bibliography.html#Xweng2021diffusion">55</a>].</span></figcaption><!--  tex4ht:label?: x41-158001r12   -->
</figure>
<!--  l. 524  --><p class="indent">       Generative models are a class of statistical models that aim to capture the probability distribution
of the data <!--  l. 524  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>x</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
in order to generate new samples. This is a challenging problem, but one that has seen
significant progress in recent years with DL, particularly in computer vision
and NLP. We will briefly walk through four popular approaches, illustrated in
Figure <a href="#summary-of-popular-generative-models-reproduced-from-ref-wengdiffusion">7.12<!--  tex4ht:ref: fig:03_ml_generative   --></a>, which can be broadly categorized as <i>likelihood-based</i> or <i>implicit</i>
models.
</p>
<h5 class="subsubsectionHead" id="likelihoodbased-models"><a id="x41-159000"></a>Likelihood-based models</h5>
<!--  l. 530  --><p class="noindent">Likelihood-based models attempt to directly learn the probability
distribution of the data through some form of (approximate) likelihood
maximization.<span class="footnote-mark"><a href="#fn36x8" id="fn36x8-bk"><sup class="textsuperscript">4</sup></a></span><a id="x41-159001f4"></a>
Flow-based models, for example, learn a series of invertible transformations to map
a simple base distribution that is easy to sample from, such as a Gaussian,
to the complex target data distribution. The most popular of these at the
time of writing are “normalizing flows”, which require each transformation to
have a tractable Jacobian determinant with which to correctly normalize the
result.
                                                                                

                                                                                
</p><!--  l. 535  --><p class="indent">       Normalizing flows have a number of advantages, such as their simple and
intuitive training objective — maximizing the likelihood of each data point — and a
tractable likelihood evaluation. These have led to successful applications to density
estimation and generation tasks in both computer vision [<a href="Bibliography.html#Xdinh2017density">256</a>, <a href="Bibliography.html#Xkingma2018glow">257</a>] and HEP [<a href="Bibliography.html#XKrause_2021ilc">258</a>].
However, the constraint of invertible transformations with tractable Jacobians turns out
to be extremely restrictive on the model design and expressivity in practice [<a href="Bibliography.html#Xfjelde2024flow">259</a>, <a href="Bibliography.html#Xhawley2024flow">260</a>],
generally resulting in worse performance on high-dimensional data compared to the
models we discuss below. Recently, over the last year, a related (in spirit) class of
models without the normalization constraint, called “flow-matching” models, have
emerged with extremely promising and, in some cases, state-of-the-art (SOTA) results
on images [<a href="Bibliography.html#Xliu2023flow">261</a>, <a href="Bibliography.html#Xlee2024improving">262</a>].
</p><!--  l. 540  --><p class="indent">       Another example of a likelihood-based model is the variational autoencoder
(VAE) [<a href="Bibliography.html#XVAE">254</a>], which is structurally similar to an AE in that it has an encoder mapping
an input data point into a latent representation, and a decoder mapped that back to the
original. They key novelty, however, is that the latent space is encouraged through the
loss function to follow a well-defined simple distribution to sample from — again,
typically, a Gaussian. Explicitly, the VAE loss function is a combination of the
reconstruction loss of a standard AE and the Kullback-Leibler divergence between the
learned latent distribution and the assumed prior. Together, this can be shown to
approximate the evidence lower bound (ELBO) of the true likelihood [<a href="Bibliography.html#XVAE">254</a>], which is
why VAEs are thought of as likelihood-based.
</p><!--  l. 545  --><p class="indent">       VAEs were one of the early success stories in generative modeling, with a
                                                                                

                                                                                
relatively simple implementation, training, and learning objective. However, they again
are restrictive, this time due to the strong assumption imposed on the latent space,
which actually competes with the reconstruction objective, and which, if incorrect,
limits the performance. Indeed, our early studies in HEP showed that the learned latent
space of VAEs is manifestly non-Gaussian for jets, leading to suboptimal performance
with a Gaussian latent prior [<a href="Bibliography.html#XOrzari_2023">263</a>]. This is why VAEs also generally yield poorer
performance than <i>generative adversarial networks</i> (GANs) [<a href="Bibliography.html#XGoodfellow_2014upx">264</a>], which we discuss
next.
</p><!--  l. 550  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="gans"><a id="x41-160000"></a>GANs</h5>
<!--  l. 552  --><p class="noindent">GANs are a type of implicit generative model. This means they learn to generate
samples without directly learning the likelihood of the data. Instead, their loss function
is effectively provided by a second neural network, called the discriminator or
critic, which tries to distinguish between real and generated samples. The two
generator and discriminator networks, with the former aiming to fool the latter, are
trained iteratively and <i>adversarially</i>, forming a feedback loop and progressively
improving each other. This continues until, ideally, the duo converge to a point
where the generator produces samples indistinguishable from the real by the
                                                                                

                                                                                
discriminator.
</p><!--  l. 558  --><p class="indent">       GANs have an interesting game-theoretic interpretation as a minimax
game, where the Nash equilibrium, or global optimum, is achieved through
minimizing the Jensen-Shannon divergence between the real and generated data
distributions [<a href="Bibliography.html#XGoodfellow_2014upx">264</a>]. Several variations of GANs have also been proposed, including the
Wasserstein-GAN [<a href="Bibliography.html#Xarjovsky2017wasserstein">265</a>], which instead aims to minimize the Wasserstein distance
between the two distributions.
</p><!--  l. 561  --><p class="indent">       Due to the adversarial nature of the training, GANs are notoriously difficult to
train [<a href="Bibliography.html#Xarjovsky2017wasserstein">265</a>–<a href="Bibliography.html#Xmescheder2018training">268</a>]. However, their formulation poses no restrictions on the form of the
generator while providing a powerful loss function and feedback mechanism. When
trained successfully, this leads to expressive, flexible, and extremely successful
generative models in a wide variety of domains. Indeed, at the time of the work of
this dissertation, GANs were the SOTA in computer vision [<a href="Bibliography.html#Xkarras2019style">269</a>–<a href="Bibliography.html#Xkarras2020analyzing">271</a>] and
had shown promising signs in HEP as well [<a href="Bibliography.html#XATL-SOFT-PUB-2018-001">222</a>, <a href="Bibliography.html#XPaganini_2017dwg">272</a><span class="ec-lmbx-12">? ? ? </span>, <a href="Bibliography.html#XATL-SOFT-PUB-2020-006">273</a>]. However,
as we highlight below, there had been no successful application of GANs, or
indeed any generative model, to point cloud data and GNNs or transformers in
HEP.
</p><!--  l. 567  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="scorebased-diffusion-models"><a id="x41-161000"></a>Score-based diffusion models</h5>
<!--  l. 569  --><p class="noindent">Finally, we briefly note the recent development in the past two years of a new class of
generative models, called <i>diffusion</i> or <i>score-based</i> models [<a href="Bibliography.html#Xho2020denoising">274</a>, <a href="Bibliography.html#Xsong2021score">275</a>]. These models
iteratively “denoise” initial Gaussian noise into something resembling samples from the
true data distribution; conceptually, this is related to diffusion in physical systems. The
breakthrough with these models came from recognizing that, with the right learning
objective, this denoising process is in fact equivalent to following the gradient of the
log-likelihood function, AKA the <i>score</i>.
</p><!--  l. 573  --><p class="indent">       Diffusion models allow a likelihood-driven training objective, like flow-based
models, but without the restrictive constraints (as the score does not need to be
normalized!), thereby offering the flexibility of a GAN along with a far more stable
training procedure. This, combined with several innovations in training and inference
techniques, has led to diffusion models surpassing GANs in computer vision [<a href="Bibliography.html#Xdhariwal2021diffusion">276</a>], and
showing promising signs over the last year in HEP as well (in part enabled by
the work in this dissertation, as we discuss in Chapter <span class="ec-lmbx-12">??</span>). However, so far,
diffusion models remain computationally expensive, with inference naively
requiring up to hundreds of denoising steps, which limits their application to fast
simulations. Nevertheless, they are an exciting area for exploration in future
work.
                                                                                

                                                                                
</p><!--  l. 578  --><p class="noindent">
</p>
<h4 class="subsectionHead" id="previous-work"><span class="titlemark">7.3.3   </span> <a id="x41-1620007.3.3"></a>Previous work</h4>
<!--  l. 583  --><p class="noindent"><i>Note: the following discussion represents the state of the field at the time of our first
publications in 2021, to provide context. Since then, the field has evolved significantly,
partly due to the work presented in this dissertation, as we discuss in Chapter ??.
</i>
</p><!--  l. 585  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="generative-modeling-in-hep"><a id="x41-163000"></a>Generative modeling in HEP</h5>
<!--  l. 587  --><p class="noindent">Past work in generative modeling in HEP exclusively used image-based representations for
HEP data. The benefit of images is the ability to employ CNN-based generative models,
which have been highly successful on computer vision tasks. References [<a href="Bibliography.html#XATL-SOFT-PUB-2018-001">222</a>, <a href="Bibliography.html#XPaganini_2017dwg">272</a><span class="ec-lmbx-12">? ? ?</span>
, <a href="Bibliography.html#XATL-SOFT-PUB-2020-006">273</a>], for example, build upon CNN-based GANs, and Ref. [<a href="Bibliography.html#Xsarm">277</a>] uses an autoregressive
model, to output jet- and detector-data-images.
</p><!--  l. 592  --><p class="indent">       However, as highlighted in Section <a href="Introduction1.html#the-importance-of-being-physicsinformed">7.1.4<!--  tex4ht:ref: sec:03_ml_physics   --></a>, the high sparsity of the images can lead
to training difficulties in GANs, while the irregular geometry of the data poses a
challenge for CNNs which require uniform matrices. While these challenges can
                                                                                

                                                                                
be mitigated to an extent with techniques such as batch normalization [<a href="Bibliography.html#Xioffe2015batch">212</a>]
and using larger/more regular pixels [<a href="Bibliography.html#XATL-SOFT-PUB-2020-006">273</a>], the approach we develop avoids
both issues by generating particle-cloud-representations of the data, which are
inherently sparse data structures and completely flexible to the underlying
geometry.
</p><!--  l. 596  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="point-cloud-generative-modeling"><a id="x41-164000"></a>Point cloud generative modeling</h5>
<a id="x41-164000doc"></a>
<figure class="figure">
<!--  l. 606  --><p class="noindent" id="sample-point-clouds-from-the-shapenet-dataset-reproduced-from-ref-dblpjournalscorrgadelhamw"><img alt="PIC" src="figures/03-ML/shapenet.png" style="max-width:100%"/> <a id="x41-164001r13"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.13. </span></span><span class="content">Sample point clouds from the ShapeNet dataset, reproduced from
Ref. [<a href="Bibliography.html#XDBLP_journals_corr_GadelhaMW17">56</a>].                                                                       </span></figcaption><!--  tex4ht:label?: x41-164001r13   -->
</figure>
<!--  l. 611  --><p class="indent">       Prior to this work, point cloud generative approaches had not yet been developed
in HEP; however, there had been some work in computer vision, primarily for 3D
objects like those from the ShapeNet dataset [<a href="Bibliography.html#Xshapenet">278</a>]. As shown in Figure <a href="#sample-point-clouds-from-the-shapenet-dataset-reproduced-from-ref-dblpjournalscorrgadelhamw">7.13<!--  tex4ht:ref: fig:03_ml_shapenet   --></a>, ShapeNet
comprises point clouds derived by sampling everyday objects in position space, and are
thus naively analogous to the particle cloud representations in momentum space we
employ for jets. However, as we note next, there are important differences in the
inductive biases of the two datasets.
</p><!--  l. 616  --><p class="indent">       Firstly, jets have physically meaningful low- and high-level features
such as particle momentum, total mass of the jet, the number of sub-jets, and
<!--  l. 616  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math>-particle
energy correlations. These physical observables are how we characterize jets, and hence
are important to reproduce correctly for physics analysis applications. Secondly, unlike
the conditional distributions of points given a particular ShapeNet object, which are
identical and independent, particle distributions within jets are highly correlated, as the
particles each originate from a single source. The independence of their constituents also
means that ShapeNet-sampled point clouds can be chosen to be of a fixed
cardinality, whereas this is not possible for jets, which inherently contain varying
numbers of particles due to the stochastic nature of particle production. Indeed,
the cardinality is correlated with other jets features such as the jet mass and
type.
                                                                                

                                                                                
</p>
<h5 class="subsubsectionHead" id="baseline-models-from-computer-vision"><a id="x41-165000"></a>Baseline models from computer vision</h5>
<!--  l. 624  --><p class="noindent">Still, particle clouds and point clouds have similarities insomuch as they represent sets
of elements in some physical space, hence we first test existing point cloud GANs as
baseline comparisons on <span class="small-caps">JetNet</span>. There are several published generative models in this
area; however, the majority exploit inductive biases specific to their respective datasets,
such as ShapeNet-based [<a href="Bibliography.html#Xpcgan">279</a>–<a href="Bibliography.html#XShapeGF">282</a>] and molecular [<a href="Bibliography.html#Xkohler20">283</a>–<a href="Bibliography.html#Xgschnet">285</a>] point clouds, which are not
appropriate for jets. A more detailed discussion, including some experimental results,
can be found in App. <span class="ec-lmbx-12">??</span>.
</p><!--  l. 628  --><p class="indent">       There do exist some more general-purpose GAN models, namely r-GAN [<a href="Bibliography.html#Xrgan">286</a>],
GraphCNN-GAN [<a href="Bibliography.html#Xgraphcnngan">287</a>], and TreeGAN [<a href="Bibliography.html#Xtreegan">288</a>], and we test these on <span class="small-caps">JetNet</span>. r-GAN uses a
fully-connected (FC) network, GraphCNN-GAN uses graph convolutions based on dynamic
<!--  l. 629  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math>-nn
graphs in intermediate feature spaces, and TreeGAN iteratively up-samples the
graphs with information passing from ancestor to descendant nodes. In terms of
discriminators, past work has used either an FC or a PointNet [<a href="Bibliography.html#Xpointnet">289</a>]-style network.
Ref. [<a href="Bibliography.html#Xwang2020rethinking">290</a>] is the first work to study point cloud discriminator design in detail and
finds amongst a number of PointNet and graph convolutional models that
PointNet-Mix, which uses both max- and average-pooled features, is the most
performant.
                                                                                

                                                                                
</p><!--  l. 633  --><p class="indent">       In Chapter <span class="ec-lmbx-12">??</span>, we apply the three aforementioned generators and FC and
PointNet-Mix discriminators to our dataset, but find jet structure is not adequately
reproduced. GraphCNN’s local convolutions make learning global structure difficult, and
while the TreeGAN and FC generator + PointNet discriminator combinations are
improvements, they are not able to learn multi-particle correlations, particularly for
the complex top quark jets, nor deal with the variable-sized light quark jets
to the extent necessary for physics applications. We thus aim to overcome
limitations of existing GANs by designing novel generator and discriminator
networks that can learn such correlations and handle variable-sized particle
clouds.
</p><!--  l. 637  --><p class="noindent">
</p>
<h5 class="subsubsectionHead" id="acknowledgements2"><a id="x41-166000"></a>Acknowledgements</h5>
<!--  l. 639  --><p class="noindent">This chapter is, in part, a reprint of the materials as they appear in R. Kansal.
“Symmetry Group Equivariant Neural Networks,” (2020); and NeurIPS, 2021, R.
Kansal; J. Duarte; H. Su; B. Orzari; T. Tomei; M. Pierini; M. Touranakou; J.-R.
Vlimant; and D. Gunopulos. Particle Cloud Generation with Message Passing
Generative Adversarial Networks. The dissertation author was the primary investigator
and author of these papers.
                                                                                

                                                                                
</p>
<nav class="crosslinks-bottom"> <a href="Equivariantneuralnetworks.html">⭠</a> <a href="DataAnalysisandStatisticalInterpretation.html">⭢</a> </nav> <div class="footnotes"><a id="x41-159002x"></a>
<!--  l. 530  --><p class="indent"> <span class="footnote-mark"><a href="#fn36x8-bk" id="fn36x8"><sup class="textsuperscript">4</sup></a></span><span class="ec-lmr-10">See the next chapter for an introduction to likelihoods.</span></p> </div><div class="footer"><p>Copyright © 2024 Raghav Kansal. All rights reserved.</p></div></main>
</body>
</html>