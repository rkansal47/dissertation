<!DOCTYPE html>

<html lang="en-US" xml:lang="en-US">
<head><title>Equivariant neural networks</title>
<meta charset="utf-8"/>
<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="main.css" rel="stylesheet" type="text/css"/>
<meta content="main.tex" name="src"/>
<script async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js" type="text/javascript"></script>
<link href="style.css" rel="stylesheet" type="text/css"/>
<link href="assets/icon.png" rel="icon" type="image/x-icon"/>
</head><body>
<nav class="TOC"><span class="mainToc"><a href="index.html"><img alt="Symmetries, QFT, &amp; The Standard Model" class="mainTocLogo" src="assets/logo.png" width="100%"/></a></span>
<span class="likepartToc"><a href="Frontmatter.html#front-matter">Front matter</a></span>
<span class="likepartToc"><a href="AbstractoftheDissertation.html#abstract-of-the-dissertation">Abstract of the Dissertation</a></span>
<span class="likepartToc"><a href="Introduction.html#introduction">Introduction</a></span>
<span class="partToc">I  <a href="TheoreticalBackground.html#theoretical-background">Theoretical Background</a></span>
<span class="partToc">II  <a href="ExperimentalBackground.html#experimental-background">Experimental Background</a></span>
<span class="partToc">III  <a href="AIMLandStatisticsBackground.html#aiml-and-statistics-background">AI/ML and Statistics Background</a></span>
<span class="chapterToc">7 <a href="MachineLearningforHEP.html#machine-learning-for-hep">Machine Learning for HEP</a></span>
<span class="sectionToc">7.1 <a href="Introduction1.html#introduction1">Introduction</a></span>
<span class="sectionToc">7.2 <a href="#equivariant-neural-networks">Equivariant neural networks</a></span>
<span class="subsectionToc">7.2.1 <a href="#equivariance">Equivariance</a></span>
<span class="subsectionToc">7.2.2               <a href="#steerable-cnns-for-equivariance">Steerable               CNNs               for
<!--  l. 230  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariance</a></span>
<span class="subsectionToc">7.2.3            <a href="#tensorfield-networks-for-equivariance">Tensor-field            networks            for
<!--  l. 232  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariance</a></span>
<span class="subsectionToc">7.2.4 <a href="#lorentzgroupequivariant-networks">Lorentz-group-equivariant networks</a></span>
<span class="sectionToc">7.3 <a href="Autoencodersandgenerativemodels.html#autoencoders-and-generative-models">Autoencoders and generative models</a></span>
<span class="chapterToc">8 <a href="DataAnalysisandStatisticalInterpretation.html#data-analysis-and-statistical-interpretation">Data Analysis and Statistical Interpretation</a></span>
<span class="partToc">IV  <a href="AcceleratingSimulationswithAI.html#accelerating-simulations-with-ai">Accelerating Simulations with AI</a></span>
<span class="partToc">V  <a href="SearchesforHighEnergyHiggsBosonPairs.html#searches-for-high-energy-higgs-boson-pairs">Searches for High Energy Higgs Boson Pairs</a></span>
<span class="partToc">VI  <a href="AIforJets.html#ai-for-jets">AI for Jets</a></span>
<span class="partToc">VII  <a href="Appendix.html#appendix">Appendix</a></span>
<span class="likepartToc"><a href="Bibliography.html#bibliography">Bibliography</a></span>
</nav>
<main class="main-content"><a class="header-link" href="https://github.com/rkansal47/dissertation" rel="noopener noreferrer" style="top: 10px; right: 12px;" target="_blank"><img alt="GitHub Repository" class="header-icon" src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" style="width: 32px; height: 32px;"/></a><a class="header-link" href="https://github.com/rkansal47/dissertation/blob/gh-pages/dissertation.pdf?raw=true" rel="noopener noreferrer" style="top: 12px; right: 54px;" target="_blank"><img alt="Download PDF" class="header-icon" src="assets/download.png" style="width: 25px; height: 25px;"/></a>
<nav class="crosslinks-top"> <a href="Introduction1.html">⭠</a> <a href="Autoencodersandgenerativemodels.html">⭢</a> </nav>
<h3 class="sectionHead" id="equivariant-neural-networks"><span class="titlemark">7.2   </span> <a id="x40-1500007.2"></a>Equivariant neural networks</h3>
<!--  l. 316  --><p class="noindent">ANNs and DL have shown remarkable success in a wide range of computer vision and
NLP tasks, motivating applications to the physical sciences. However, as highlighted in
the previous section, the power of DL models is often derived from architectures tuned
to the inductive biases of their domains.
</p><!--  l. 319  --><p class="indent">       A unique feature of physical data is its inherent physical <i>symmetries</i> (see Chapter <a href="Symmetriesinphysics.html#symmetries-in-physics">2<!--  tex4ht:ref: sec:01_symmetries   --></a>), such as
with respect to <!--  l. 319  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
and Lorentz-transformations for molecules and high-energy collisions, respectively. It is
hence desirable to develop NN architectures that themselves are intrinsically <i>equivariant</i>
to the associated transformations, which can thereby be more data efficient, more easily
interpretable, and perhaps ultimately more successful [<a href="Bibliography.html#Xthomas2018tensor">224</a>].
</p><!--  l. 322  --><p class="indent">       We have already encountered some forms of equivariance: to translations in
CNNs and to permutations in GNNs. More recently, there has been work on building
equivariance to a broader set of transformations, such as the symmetries mentioned
above, which will be the focus of this section.
</p><!--  l. 327  --><p class="noindent">
</p>
<h4 class="subsectionHead" id="equivariance"><span class="titlemark">7.2.1   </span> <a id="x40-1510007.2.1"></a>Equivariance</h4>
<!--  l. 330  --><p class="noindent">Let us first introduce precisely what we mean by “equivariance”, adapting a definition
from Refs. [<a href="Bibliography.html#Xcohen2016group">225</a>–<a href="Bibliography.html#Xweiler2019general">228</a>].
</p>
<div class="newtheorem">
<!--  l. 332  --><p class="noindent"><span class="head">
<a id="x40-151001r1"></a>
<span class="ec-lmbx-12">Definition 7.2.1.</span>
</span> A feature map <!--  l. 334  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi> <mo class="MathClass-punc" stretchy="false">:</mo> <mstyle mathvariant="script"><mi>X</mi></mstyle> <mo class="MathClass-rel" stretchy="false">→</mo><mstyle mathvariant="script"><mi>Y</mi> </mstyle></mrow></math>
is considered <strong>equivariant</strong> to a group of transformations
<!--  l. 334  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math> if
<!--  l. 334  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi class="MathClass-op">∀</mi><mo> ⁡<!--  FUNCTION APPLICATION  --></mo><mi>g</mi> <mo class="MathClass-rel" stretchy="false">∈</mo> <mi>G</mi></mrow></math> and some
representation <!--  l. 334  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math> there
exists a representation <!--  l. 334  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup></math>
satisfying </p><table class="equation"><tr><td>
<!--  l. 335  --><p class="indent">
</p><!--  l. 335  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-151002r1"></mstyle><!--  endlabel  -->
<msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup><mo class="MathClass-open" stretchy="false">(</mo><mi>g</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>x</mi><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>π</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>g</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>x</mi><mo class="MathClass-close" stretchy="false">)</mo><mo class="MathClass-punc" stretchy="false">,</mo>
</mrow></math></td><td class="eq-no">(7.2.1)</td></tr></table>
<!--  l. 337  --><p class="noindent">i.e. the group operation commutes with the map
<!--  l. 338  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> (and
<!--  l. 338  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> therefore is an intertwiner).
In this context, <!--  l. 339  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>
generally represents a NN layer. Another way to think about this is that each transformation by a
group element <!--  l. 340  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math>
on the input must correspond to a transformation by the same group
element in the feature space (but with potentially different representations
<!--  l. 340  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math> and
<!--  l. 340  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup></math>).
</p>
</div>
<!--  l. 341  --><p class="indent">
</p>
<div class="newtheorem">
<!--  l. 343  --><p class="noindent"><span class="head">
<a id="x40-151003r2"></a>
<span class="ec-lmbx-12">Definition 7.2.2.</span>
</span><strong>Invariance</strong>      is       the       particular       case       where
<!--  l. 344  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup></math>
is the trivial representation (<!--  l. 344  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup><mo class="MathClass-open" stretchy="false">(</mo><mi>g</mi><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <mi>𝟙</mi> </mrow></math> ),
wherein transformations on <!--  l. 344  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math>
do not affect features at all.
</p>
</div>
<!--  l. 345  --><p class="indent">
</p><!--  l. 347  --><p class="indent">       While for many tasks, such as classification, <i>invariance</i> of the outputs is sufficient,
Refs. [<a href="Bibliography.html#Xcohen2016steerable">226</a>, <a href="Bibliography.html#Xworrall2017harmonic">227</a>] argue that equivariance is more desirable at least in the intermediate
layers, as it allows the network to learn useful information about the transformation
<!--  l. 347  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math>
itself.
</p><!--  l. 349  --><p class="indent">       So far, we have discussed CNNs and GNNs / transformers, which are equivariant to the
<!--  l. 349  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>T</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mi>N</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> group (translations
in <!--  l. 349  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math> dimensions) and
invariant to the <!--  l. 349  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="normal"><mi>S</mi></mstyle></mrow><mrow><mi>N</mi></mrow></msub></math> group
(permutations of <!--  l. 349  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math>
                                                                                

                                                                                
objects), respectively. Next, we discuss the extension to broader symmetry
groups.
</p><!--  l. 353  --><p class="noindent">
</p>
<h4 class="subsectionHead" id="steerable-cnns-for-equivariance"><span class="titlemark">7.2.2   </span> <a id="x40-1520007.2.2"></a>Steerable  CNNs  for
<!--  l. 353  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariance</h4>
<!--  l. 356  --><p class="noindent">We first describe the generalization of the translational invariance of CNNs to
equivariance to not only translations, but <i>rotations and reflections</i> in 2D as well; i.e, the
<!--  l. 356  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> group. We make
use of a general procedure, based on Refs. [<a href="Bibliography.html#Xcohen2016group">225</a>, <a href="Bibliography.html#Xcohen2016steerable">226</a>], for extending 2D translational invariance
(<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>T</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>) to equivariance
to a group <!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <mstyle mathvariant="normal"><mi>T</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-bin" stretchy="false">⋊</mo> <mi>H</mi></mrow></math>, where
<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mo class="MathClass-bin" stretchy="false">⋊</mo></math> is the semi-direct product
and <!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math> is a subgroup of
<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math>, meaning we can induce
representations of <!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math>,
<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mrow><mstyle mathvariant="normal"><mi>Ind</mi></mstyle></mrow><mrow><mi>H</mi></mrow><mrow><mi>G</mi></mrow></msubsup></math>, from
<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math>.<span class="footnote-mark"><a href="#fn33x8" id="fn33x8-bk"><sup class="textsuperscript">1</sup></a></span><a id="x40-152001f1"></a>
For <!--  l. 358  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>, in
                                                                                

                                                                                
particular, <!--  l. 358  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>H</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <mstyle mathvariant="normal"><mi>O</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>,
the group of distance-preserving transformations in 2D; i.e., rotations and
reflections.
</p><!--  l. 360  --><p class="indent">       The  key  idea  in  developing  a
<!--  l. 360  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math>-equivariant layer is to first find
the set of maps <!--  l. 360  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi> <mo class="MathClass-rel" stretchy="false">∋</mo> <mi>f</mi></mrow></math> which satisfy
Eq. <a href="#x40-151002r1">7.2.1<!--  tex4ht:ref: eq:06_equivariantnns_equiv   --></a> for an element <!--  l. 360  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi> <mo class="MathClass-rel" stretchy="false">∈</mo> <mi>H</mi></mrow></math>:
</p><table class="equation"><tr><td>
<!--  l. 361  --><p class="indent">
</p><!--  l. 361  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-152003r2"></mstyle><!--  endlabel  -->
<msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>out</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mi>h</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <mi>f</mi><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>in</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mi>h</mi><mo class="MathClass-close" stretchy="false">)</mo>
</mrow></math></td><td class="eq-no">(7.2.2)</td></tr></table>
<!--  l. 363  --><p class="noindent">where <!--  l. 364  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>out</mi></mstyle></mrow></msub></math>
and <!--  l. 364  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>in</mi></mstyle></mrow></msub></math> are
reps of <!--  l. 364  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math>.
After this, Eq. <a href="#x40-151002r1">7.2.1<!--  tex4ht:ref: eq:06_equivariantnns_equiv   --></a> can be automatically satisfied using </p><table class="equation"><tr><td>
<!--  l. 365  --><p class="indent">
</p><!--  l. 365  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-152004r3"></mstyle><!--  endlabel  -->
<msup><mrow><mi>π</mi></mrow><mrow><mi>′</mi></mrow></msup><mo class="MathClass-open" stretchy="false">(</mo><mi>g</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi> <mo class="MathClass-rel" stretchy="false">=</mo><msubsup><mrow> <mstyle mathvariant="normal"><mi>Ind</mi></mstyle></mrow><mrow>
<mi>H</mi></mrow><mrow><mi>G</mi></mrow></msubsup><mo class="MathClass-open" stretchy="false">(</mo><mi>g</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal">
<mi>out</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mi>h</mi><mo class="MathClass-close" stretchy="false">)</mo><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>in</mi></mstyle></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><msup><mrow><mi>h</mi></mrow><mrow><mo class="MathClass-bin" stretchy="false">−</mo><mn>1</mn></mrow></msup><mo class="MathClass-close" stretchy="false">)</mo><mo class="MathClass-open" stretchy="false">(</mo><mi>x</mi> <mo class="MathClass-bin" stretchy="false">−</mo> <mi>t</mi><mo class="MathClass-close" stretchy="false">)</mo><mo class="MathClass-close" stretchy="false">)</mo>
</mrow></math></td><td class="eq-no">(7.2.3)</td></tr></table>
<!--  l. 367  --><p class="noindent">where <!--  l. 368  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi> <mo class="MathClass-rel" stretchy="false">=</mo> <mi mathvariant="italic">th</mi></mrow></math> for some
2D translation <!--  l. 368  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi> <mo class="MathClass-rel" stretchy="false">∈</mo><mstyle mathvariant="normal"><mi>T</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>.
</p><!--  l. 371  --><p class="indent">       Since Eq. <a href="#x40-152003r2">7.2.2<!--  tex4ht:ref: eq:06_equivariantnns_equiv2   --></a> is linear in <!--  l. 371  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>,
we want a complete linear basis of functions that satisfy it. We can obtain
this by restricting the convolutional filters of a standard CNN to circular
harmonics [<a href="Bibliography.html#Xworrall2017harmonic">227</a>]:<span class="footnote-mark"><a href="#fn34x8" id="fn34x8-bk"><sup class="textsuperscript">2</sup></a></span><a id="x40-152005f2"></a>
</p><table class="equation"><tr><td>
<!--  l. 373  --><p class="indent">
</p><!--  l. 373  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-152007r4"></mstyle><!--  endlabel  -->
<msub><mrow><mi>W</mi></mrow><mrow><mi>m</mi></mrow></msub><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>ϕ</mi><mo class="MathClass-punc" stretchy="false">;</mo> <mi>R</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>β</mi><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <mi>R</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-close" stretchy="false">)</mo><msup><mrow><mi>e</mi></mrow><mrow><mi>i</mi><mo class="MathClass-open" stretchy="false">(</mo><mi mathvariant="italic">mϕ</mi><mo class="MathClass-bin" stretchy="false">+</mo><mi>β</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></msup><mo class="MathClass-punc" stretchy="false">,</mo>
</mrow></math></td><td class="eq-no">(7.2.4)</td></tr></table>
<!--  l. 375  --><p class="noindent">where the radial component <!--  l. 376  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math> and
the filter phase <!--  l. 376  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math> are learnable
parameters. We can see that <!--  l. 377  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi> <mo class="MathClass-rel" stretchy="false">∈</mo> <mi>ℤ</mi></mrow></math>,
these filters form a complete basis and satisfy Eq. <a href="#x40-152003r2">7.2.2<!--  tex4ht:ref: eq:06_equivariantnns_equiv2   --></a> under convolutions
(<!--  l. 377  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mo class="MathClass-bin" stretchy="false">∗</mo></math>) with an
image <!--  l. 377  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>ϕ</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
rotated by <!--  l. 377  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>𝜃</mi></math>:
</p><table class="equation"><tr><td>
<!--  l. 378  --><p class="indent">
</p><!--  l. 378  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-152008r5"></mstyle><!--  endlabel  -->
<msub><mrow><mi>W</mi></mrow><mrow><mi>m</mi></mrow></msub> <mo class="MathClass-bin" stretchy="false">∗</mo> <mi>F</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>ϕ</mi> <mo class="MathClass-bin" stretchy="false">+</mo> <mi>𝜃</mi><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <msup><mrow><mi>e</mi></mrow><mrow><mi mathvariant="italic">im𝜃</mi></mrow></msup><msub><mrow><mi>W</mi></mrow><mrow>
<mi>m</mi></mrow></msub> <mo class="MathClass-bin" stretchy="false">∗</mo> <mi>F</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-punc" stretchy="false">,</mo><mi>ϕ</mi><mo class="MathClass-close" stretchy="false">)</mo><mo class="MathClass-punc" stretchy="false">.</mo>
</mrow></math></td><td class="eq-no">(7.2.5)</td></tr></table>
<!--  l. 380  --><p class="noindent">Here we took <!--  l. 381  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>in</mi></mstyle></mrow></msub></math> to be
the fundamental <!--  l. 381  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>SO</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> rep
acting on the image and <!--  l. 381  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>ρ</mi></mrow><mrow><mstyle mathvariant="normal"><mi>out</mi></mstyle></mrow></msub></math>
to be any one of the infinite complex reps. After discretizing these filters
Ref. [<a href="Bibliography.html#Xworrall2017harmonic">227</a>] demonstrates significant improvement in classification of rotated
images compared to SOTA CNNs. Such networks are generally referred to
as “Steerable CNNs”, and, in practice, are implemented using a finite set of
<!--  l. 383  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math> such circular
harmonic filters, with <!--  l. 383  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi> <mo class="MathClass-rel" stretchy="false">∈</mo><mo class="MathClass-open" stretchy="false">{</mo><mn>0</mn><mo class="MathClass-punc" stretchy="false">,</mo> <mfrac><mrow><mn>2</mn><mi>π</mi></mrow>
<mrow>
<mi>N</mi></mrow></mfrac> <mo class="MathClass-punc" stretchy="false">,</mo><mo class="MathClass-punc" stretchy="false">.</mo><mo class="MathClass-punc" stretchy="false">.</mo><mo class="MathClass-punc" stretchy="false">.</mo><mo class="MathClass-punc" stretchy="false">,</mo> <mfrac><mrow><mn>2</mn><mi>π</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>N</mi><mo class="MathClass-bin" stretchy="false">−</mo><mn>1</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow>
<mrow>
<mi>N</mi></mrow></mfrac> <mo class="MathClass-close" stretchy="false">}</mo></mrow></math>
(and possibly their reflections), which are then pooled in a rotationally-invariant
manner, as illustrated in Figure <a href="#schematic-of-a-steerable-cnn-reproduced-from-ref-weilerlearning">7.8<!--  tex4ht:ref: fig:06_equivariantnns_scnn   --></a>.
</p>
<figure class="figure">
<!--  l. 388  --><p class="noindent" id="schematic-of-a-steerable-cnn-reproduced-from-ref-weilerlearning"><img alt="PIC" src="figures/06-ML4Jets/equivariantnns/scnns.png" style="max-width:100%"/> <a id="x40-152009r8"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.8. </span></span><span class="content">Schematic of a steerable CNN, reproduced from Ref. [<a href="Bibliography.html#Xweiler2018learning">51</a>].         </span></figcaption><!--  tex4ht:label?: x40-152009r8   -->
</figure>
<h4 class="subsectionHead" id="tensorfield-networks-for-equivariance"><span class="titlemark">7.2.3   </span> <a id="x40-1530007.2.3"></a>Tensor-field  networks  for
<!--  l. 394  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariance</h4>
<!--  l. 397  --><p class="noindent">Steerable CNNs have been extended to
<!--  l. 397  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariance
— translations, rotations, and reflections in 3D — as well [<a href="Bibliography.html#Xweiler20183d">229</a>]. However, we will
discuss a slightly different approach, applied to point-cloud data. This approach
uses “Fourier decompositions” of the input, feature, and output spaces into
irreducible representations (irreps) of the symmetry group, and is referred
to as a “tensor-field network” [<a href="Bibliography.html#Xthomas2018tensor">224</a>]. In addition to their aforementioned
applications to HEP, point clouds are also extremely useful representations of
physical objects such as molecules and crystals, both of which are inherently
<!--  l. 400  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
invariant.
</p><!--  l. 402  --><p class="indent">       In the approach of Ref. [<a href="Bibliography.html#Xthomas2018tensor">224</a>], the input and intermediate network layers
<!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> take the set of
coordinates <!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>r</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math>
and features <!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math>
for each point <!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi></math>
                                                                                

                                                                                
in the point cloud and map them to the same set of coordinates with new learned features
<!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>y</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math>
(<!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><msub><mrow><mstyle mathvariant="bold"><mi>r</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub><mo class="MathClass-punc" stretchy="false">,</mo><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub><mo class="MathClass-close" stretchy="false">)</mo> <mo class="MathClass-rel" stretchy="false">=</mo> <mo class="MathClass-open" stretchy="false">(</mo><msub><mrow><mstyle mathvariant="bold"><mi>r</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub><mo class="MathClass-punc" stretchy="false">,</mo><msub><mrow><mstyle mathvariant="bold"><mi>y</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>), with an
equivariant <!--  l. 402  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>
again having to satisfy Eq. <a href="#x40-151002r1">7.2.1<!--  tex4ht:ref: eq:06_equivariantnns_equiv   --></a>. If necessary, the features are aggregated at the end across
all points to produce the output. Translation equivariance is achieved directly by requiring
<!--  l. 404  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> to only consider
distances <!--  l. 404  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mrow><mstyle mathvariant="bold"><mi>r</mi></mstyle></mrow><mrow><mi>i</mi></mrow></msub> <mo class="MathClass-bin" stretchy="false">−</mo><mstyle mathvariant="bold"><msub><mrow><mi>r</mi></mrow><mrow><mi>j</mi></mrow></msub></mstyle></mrow></math>
between points <!--  l. 404  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math>
and <!--  l. 404  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math>
(a global translation will not affect these).
</p><!--  l. 406  --><p class="indent">       For rotation equivariance, first the feature vectors
<!--  l. 406  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math>
are decomposed according to how they transform under irreps of
<!--  l. 406  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>SO</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
— scalars, vectors or higher order tensors (the coordinates
<!--  l. 406  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>r</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math> already transform
as vectors in <!--  l. 406  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>ℝ</mi></mrow><mrow><mn>3</mn></mrow></msup></math>
under the fundamental rep): </p><table class="equation"><tr><td>
<!--  l. 407  --><p class="indent">
</p><!--  l. 407  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-153001r6"></mstyle><!--  endlabel  -->
<msup><mrow><mi>ℝ</mi></mrow><mrow><mn>3</mn></mrow></msup> <mo class="MathClass-bin" stretchy="false">⊕</mo><mstyle mathvariant="script"><mi>X</mi></mstyle> <mo class="MathClass-rel" stretchy="false">=</mo><munder class="msub"><mrow><mo> ⊕</mo>
</mrow><mrow><mi>l</mi></mrow></munder><msubsup><mrow><mi>R</mi></mrow><mrow><mi>l</mi></mrow><mrow><msub><mrow><mi>m</mi></mrow><mrow><mi>l</mi></mrow></msub>
</mrow></msubsup>
</mrow></math></td><td class="eq-no">(7.2.6)</td></tr></table>
<!--  l. 409  --><p class="noindent">where the sum is performed over irreps <!--  l. 410  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>R</mi></mrow><mrow><mi>l</mi></mrow></msub></math>
(with dimension <!--  l. 410  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mi>l</mi> <mo class="MathClass-bin" stretchy="false">+</mo> <mn>1</mn></mrow></math>)
and <!--  l. 410  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>m</mi></mrow><mrow><mi>l</mi></mrow></msub></math>
are the multiplicities. Thus, each point’s features and coordinates have the
corresponding decomposition: </p><table class="equation"><tr><td>
<!--  l. 412  --><p class="indent">
</p><!--  l. 412  --><math class="equation" display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow>
<mstyle class="label" id="x40-153002r7"></mstyle><!--  endlabel  --><msub><mrow>
<mstyle mathvariant="bold">
<mi>r</mi></mstyle>
</mrow><mrow><mi>a</mi></mrow></msub>
<mo class="MathClass-bin" stretchy="false">⊕</mo><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub> <mo class="MathClass-rel" stretchy="false">=</mo><munder class="msub"><mrow><mo> ⊕</mo>
</mrow><mrow><mi>l</mi></mrow></munder><munderover accent="false" accentunder="false"><mrow><mo> ⊕</mo>
</mrow><mrow><mi>c</mi><mo class="MathClass-rel" stretchy="false">=</mo><mn>1</mn></mrow><mrow><msub><mrow><mi>m</mi></mrow><mrow><mi>l</mi></mrow></msub>
</mrow></munderover><msubsup><mrow><mi>V</mi> </mrow><mrow><mi mathvariant="italic">ac</mi></mrow><mrow><mi>l</mi></mrow></msubsup>
</mrow></math></td><td class="eq-no">(7.2.7)</td></tr></table>
<!--  l. 414  --><p class="noindent">where the <!--  l. 415  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mrow><mi>V</mi> </mrow><mrow><mi mathvariant="italic">ac</mi></mrow><mrow><mi>l</mi></mrow></msubsup></math> are tensors
which transform under the <!--  l. 415  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>l</mi></math>
irrep. Similar to steerable CNNs, each of these tensors are individually
                                                                                

                                                                                
acted upon by generalized convolutional filters with the form
<!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>r</mi><mo class="MathClass-close" stretchy="false">)</mo><msup><mrow><mi>Y</mi> </mrow><mrow><msub><mrow><mi>l</mi></mrow><mrow><mi>f</mi></mrow></msub></mrow></msup><mo class="MathClass-open" stretchy="false">(</mo><mover accent="true"><mrow><mi>r</mi></mrow><mo accent="true">^</mo></mover><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>, where
<!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math> is a learned radial function,
<!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>Y</mi> </mrow><mrow><mi>l</mi></mrow></msup></math> are the spherical harmonic
tensors, and the set <!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>l</mi></mrow><mrow><mi>f</mi></mrow></msub></math>
corresponds to the set of desired irreps in feature space. The spherical
harmonics are directly analogous to using circular harmonics for
<!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> (except they have
dimension <!--  l. 416  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mi>l</mi> <mo class="MathClass-bin" stretchy="false">+</mo> <mn>1</mn></mrow></math>) and
by the same argument they satisfy Eq. <a href="#x40-151002r1">7.2.1<!--  tex4ht:ref: eq:06_equivariantnns_equiv   --></a>. This convolution effectively produces a tensor product
representation of <!--  l. 417  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>SO</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
<!--  l. 417  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mrow><mi>R</mi></mrow><mrow><mi>l</mi></mrow></msub> <mo class="MathClass-bin" stretchy="false">⊗</mo> <msub><mrow><mi>R</mi></mrow><mrow><msub><mrow><mi>l</mi></mrow><mrow><mi>f</mi></mrow></msub></mrow></msub></mrow></math>,
which is then decomposed via Clebsch-Gordan (CG) decomposition into irreps
again.
</p><!--  l. 419  --><p class="indent">       A useful pedagogical example is of a network taking as input
a collection of point masses and outputting the inertia tensor. The
input features are the masses of each point, which are scalars under
<!--  l. 420  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>SO</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>, and the inertia tensor
transforms as the <!--  l. 420  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn> <mo class="MathClass-bin" stretchy="false">⊕</mo> <mn>2</mn></mrow></math>
representation, so we define this network to be of the type
<!--  l. 420  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn> <mo class="MathClass-rel" stretchy="false">→</mo> <mn>0</mn> <mo class="MathClass-bin" stretchy="false">⊕</mo> <mn>2</mn></mrow></math>.
</p><!--  l. 422  --><p class="indent">       Some interesting and successful applications include classifying molecules [<a href="Bibliography.html#Xmiller2020relevance">230</a>],
                                                                                

                                                                                
predicting protein complex structures [<a href="Bibliography.html#XEismann2020HierarchicalRN">231</a>], and predicting the phonon density of states
(DoS) in crystals [<a href="Bibliography.html#Xchen2020direct">52</a>]. A schematic of the architecture used for the latter is shown in
Figure <a href="#schematic-of-the-equivariant-neural-network-architecture-used-for-predicting-phonon-density-of-states-reproduced-from-ref-chendirect">7.9<!--  tex4ht:ref: fig:06_equivariantnns_dos   --></a>. Different crystals are represented geometrically as point clouds in
<!--  l. 424  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>ℝ</mi></mrow><mrow><mn>3</mn></mrow></msup></math>, with individual atoms
labeled via feature vectors <!--  l. 424  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mstyle mathvariant="bold"><mi>x</mi></mstyle></mrow><mrow><mi>a</mi></mrow></msub></math>
using mass weighted one-hot encoding. After a series of convolution layers the
features are summed over all points to predict 51 scalars comprising the phonon
DoS.
</p>
<figure class="figure">
<!--  l. 430  --><p class="noindent" id="schematic-of-the-equivariant-neural-network-architecture-used-for-predicting-phonon-density-of-states-reproduced-from-ref-chendirect"><img alt="PIC" src="figures/06-ML4Jets/equivariantnns/dos.png" style="max-width:100%"/> <a id="x40-153003r9"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.9. </span></span><span class="content">Schematic of the <!--  l. 431  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariant
neural  network  architecture  used  for  predicting  phonon  density  of  states,
reproduced from Ref. [<a href="Bibliography.html#Xchen2020direct">52</a>].                                                      </span></figcaption><!--  tex4ht:label?: x40-153003r9   -->
</figure>
<h4 class="subsectionHead" id="lorentzgroupequivariant-networks"><span class="titlemark">7.2.4   </span> <a id="x40-1540007.2.4"></a>Lorentz-group-equivariant networks</h4>
<!--  l. 439  --><p class="noindent">Recently there has been some success in creating Lorentz-group-equivariant networks,
which are desirable for DL applications to high energy data. The Lorentz group
<!--  l. 440  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>O</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-punc" stretchy="false">,</mo> <mn>1</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
comprises the set of linear transformations between inertial frames with coincident
origins. Henceforth, we restrict ourselves to the special orthochronous Lorentz group
<!--  l. 441  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mrow><mstyle mathvariant="normal"><mi>SO</mi></mstyle></mrow><mrow><mo class="MathClass-bin" stretchy="false">+</mo></mrow></msup><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-punc" stretchy="false">,</mo> <mn>1</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>,
which consists of all Lorentz transformations that preserve the orientation and direction
of time. Equivariance to such transformations is a fundamental symmetry of the data
collected out of high-energy particle collisions.
</p><!--  l. 444  --><p class="indent">       To our knowledge, there has been no generalization of steerable CNNs to the Lorentz
group; however, Refs. [<a href="Bibliography.html#Xbogatskiy2020lorentz">53</a>, <a href="Bibliography.html#Xequivariance-Fourier-Kondor">232</a>–<a href="Bibliography.html#XE_2_-Equivariant">234</a>] propose an alternative, completely Fourier-based
approach, again acting on point clouds, that shares some similarities with the
<!--  l. 444  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>-equivariant
network discussed above.
</p><!--  l. 446  --><p class="indent">       The general method is to:
</p><!--  l. 449  --><p class="indent">
</p>
<dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
1. 
</dt><dd class="enumerate-enumitem">Decompose the input space into irreps of the group.
</dd>
<dt class="enumerate-enumitem">
2. 
</dt><dd class="enumerate-enumitem">Apply an equivariant mapping (satisfying Eq. <a href="#x40-151002r1">7.2.1<!--  tex4ht:ref: eq:06_equivariantnns_equiv   --></a>) to the feature space.
</dd>
<dt class="enumerate-enumitem">
3. 
</dt><dd class="enumerate-enumitem">Take tensor products of the irreps and CG-decompose them again into irreps.
</dd>
<dt class="enumerate-enumitem">
4. 
</dt><dd class="enumerate-enumitem">Repeat steps 2–3 until the output layer.</dd></dl>
<!--  l. 455  --><p class="indent">       The crucial difference between this and the previous networks is that the mapping is
no longer via convolutional filters; instead, it is chosen to be a simple linear aggregation
across the nodes of the point clouds. Recall from Definition <a href="#x40-151001r1">7.2.1<!--  tex4ht:ref: def:06_equivariantnns_equiv   --></a> that equivariant maps
<!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> must
be intertwiners between input and output representations, which, according
to Schur’s Lemma, imposes strong restrictions on both the form of a linear
<!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> and its
output <!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo class="MathClass-open" stretchy="false">(</mo><mi>x</mi><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>.
Namely: the outputs and inputs must have the same irrep decomposition (though the
multiplicities are allowed to vary, akin to increasing/decreasing the “channels” in an image)
and <!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math> must
be a direct sum of learned matrices acting individually on each irrep. The transformation
between <!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>f</mi></mrow><mrow><mstyle mathvariant="normal"><mi>in</mi></mstyle></mrow></msup></math>
and <!--  l. 456  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi>f</mi></mrow><mrow><mstyle mathvariant="normal"><mi>pre</mi></mstyle></mrow></msup></math>
in Figure <a href="#schematic-of-a-lorentz-groupequivariant-network-layer-reproduced-from-ref-bogatskiylorentz">7.10<!--  tex4ht:ref: fig:06_equivariantnns_equivneuron   --></a> illustrates such a mapping.
                                                                                

                                                                                
</p>
<figure class="figure">
<!--  l. 461  --><p class="noindent" id="schematic-of-a-lorentz-groupequivariant-network-layer-reproduced-from-ref-bogatskiylorentz"><img alt="PIC" src="figures/06-ML4Jets/equivariantnns/equivneuron.png" style="max-width:70%"/> <a id="x40-154005r10"></a>
</p>
<figcaption class="caption"><span class="id"><span class="ec-lmbx-12">Figure 7.10. </span></span><span class="content">Schematic of a Lorentz group-equivariant network layer, reproduced
from Ref. [<a href="Bibliography.html#Xbogatskiy2020lorentz">53</a>].                                                                 </span></figcaption><!--  tex4ht:label?: x40-154005r10   -->
</figure>
<!--  l. 467  --><p class="indent">       To inject non-linearities into the network, Ref. [<a href="Bibliography.html#Xbogatskiy2020lorentz">53</a>] proposes to take tensor
products between each pair of irreps after the mapping, and then perform a CG
decomposition.<span class="footnote-mark"><a href="#fn35x8" id="fn35x8-bk"><sup class="textsuperscript">3</sup></a></span><a id="x40-154006f3"></a>
Another freedom available to us is acting with arbitrary learned functions on any
scalar irreps that result from the decomposition, since they are, by definition,
Lorentz-invariants.
</p><!--  l. 470  --><p class="indent">       One successful application of this network has been to jet tagging: Ref. [<a href="Bibliography.html#Xbogatskiy2020lorentz">53</a>]
successfully applied this “Lorentz-group network” (LGN) to top-quark identification,
demonstrating a high (92.9%) accuracy, though they were unable to match the
then-SOTA (93.8% using the ParticleNet GNN [<a href="Bibliography.html#XQu_2019gqs">223</a>]).
</p><!--  l. 473  --><p class="indent">       Finally, we note that overall this is, in fact, a very general approach:
applicable to any symmetry group. This includes the aforementioned
<!--  l. 474  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math> and
<!--  l. 474  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>3</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math>
groups as well as potentially more exotic groups such as
<!--  l. 474  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>E</mi></mrow><mrow><mn>8</mn></mrow></msub></math> or
<!--  l. 474  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi>G</mi></mrow><mrow><mn>2</mn></mrow></msub></math> which
also arise in physics. The only group-dependent operations in such a network are the
decompositions into irreps which can readily be calculated for any group (as opposed to
steerable CNNs where one needs to derive group equivariant kernels/convolutional
filters).
                                                                                

                                                                                
</p>
<h5 class="subsubsectionHead" id="summary"><a id="x40-155000"></a>Summary</h5>
<a id="x40-155000doc"></a>
<!--  l. 480  --><p class="noindent">We reviewed three approaches to creating neural networks that are equivariant to
physical symmetry groups: by extending the translation-equivariant convolutions in
CNNs to more general symmetries with appropriately defined learnable filters as in
Refs. [<a href="Bibliography.html#Xcohen2016group">225</a>, <a href="Bibliography.html#Xequivariance-kernel-Cohen">236</a>, <a href="Bibliography.html#Xequivariance-kernel-Finzi">237</a>], by operating in the Fourier space of the group [<a href="Bibliography.html#Xbogatskiy2020lorentz">53</a>], and a
combination thereof [<a href="Bibliography.html#Xthomas2018tensor">224</a>]. Such networks are highly relevant to the physical sciences,
where datasets often possess intrinsic symmetries, and, as demonstrated in some
example tasks, they are promising alternatives and improvements to standard
non-equivariant DL approaches. In particular, Lorentz-equivariant networks have shown
promise in jet classification, a key task in HEP. In Chapter <span class="ec-lmbx-12">??</span>, we will discuss the
extension of these ideas to the first Lorentz-equivariant <i>autoencoder</i> for jets, with
applications to data compression, anomaly detection, and potentially fast simulations as
well.
                                                                                

                                                                                
</p>
<nav class="crosslinks-bottom"> <a href="Introduction1.html">⭠</a> <a href="Autoencodersandgenerativemodels.html">⭢</a> </nav> <div class="footnotes"><a id="x40-152002x7.2.2"></a>
<!--  l. 357  --><p class="indent"> <span class="footnote-mark"><a href="#fn33x8-bk" id="fn33x8"><sup class="textsuperscript">1</sup></a></span><span class="ec-lmr-10">See e.g. Chapter IV, p297 of Ref. [</span><a href="Bibliography.html#XZee_2016fuk"><span class="ec-lmr-10">74</span></a><span class="ec-lmr-10">] for induced representations of</span>
<!--  l. 357  --><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathvariant="normal"><mi>E</mi></mstyle><mo class="MathClass-open" stretchy="false">(</mo><mn>2</mn><mo class="MathClass-close" stretchy="false">)</mo></mrow></math><span class="ec-lmr-10">.</span></p><a id="x40-152006x7.2.2"></a>
<!--  l. 372  --><p class="indent"> <span class="footnote-mark"><a href="#fn34x8-bk" id="fn34x8"><sup class="textsuperscript">2</sup></a></span><span class="ec-lmr-10">See Refs. [</span><a href="Bibliography.html#Xweiler2019general"><span class="ec-lmr-10">228</span></a><span class="ec-lmr-10">, </span><a href="Bibliography.html#Xweiler20183d"><span class="ec-lmr-10">229</span></a><span class="ec-lmr-10">] for a more rigorous derivation.</span></p><a id="x40-154007x7.2.4"></a>
<!--  l. 467  --><p class="indent"> <span class="footnote-mark"><a href="#fn35x8-bk" id="fn35x8"><sup class="textsuperscript">3</sup></a></span><span class="ec-lmr-10">See Ref. [</span><a href="Bibliography.html#Xgelfand2018representations"><span class="ec-lmr-10">235</span></a><span class="ec-lmr-10">] for a detailed analysis of CG decomposition for the Lorentz
group.</span></p> </div><div class="footer"><p>Copyright © 2024 Raghav Kansal. All rights reserved.</p></div></main>
</body>
</html>